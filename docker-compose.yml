version: '3'

services: 
# build kafka container
  kafka:
    container_name: stock-analysis-app-kafka-container
    image: 'bitnami/kafka:latest'
    ports:
      - '9092:9092'
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
    networks:
      - stock_network

# create kafka topic depend on kafka container
# make sure that you run (chmod +x create_topic.sh) from the directory STOCK_ANALYSIS_APP before run docker-compose
# run this code after (docker compose up) to check list of Topics (sudo docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092)

  kafka-create-topic: 
    container_name: stock-analysis-app-kafka-create-topic-container
    image: bitnami/kafka:latest 
    networks: 
      - stock_network 
    command: [ "/bin/bash", "-c", "/create_topic.sh"] 
    environment: 
      - DEFAULT_TOPIC=default_topic
    depends_on: 
      kafka: 
        condition: service_started 
    volumes: 
      - type: bind 
        source: ./create_topic.sh 
        target: /create_topic.sh 
    init: true

#build Hadoop
  namenode:
    container_name: stock-analysis-app-namenode
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  datanode:
    container_name: stock-analysis-app-datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./config      
  resourcemanager:
    container_name: stock-analysis-app-resourcemanager
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
        - 8088:8088
    env_file:
      - ./config
    volumes:
      - ./test.sh:/opt/test.sh
  nodemanager:
    container_name: stock-analysis-app-nodemanager
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    env_file:
      - ./config


  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    environment:
      HIVE_DB: metastore
      HIVE_USER: hive
      HIVE_PASSWORD: hive
      HIVE_HOST: localhost
      HADOOP_HOME: /opt/hadoop
      HIVE_HOME: /opt/hive
    ports:
      - "9083:9083"
    depends_on:
      - namenode
      - datanode
    networks:
      - stock_network

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    environment:
      SERVICE_PRECONDITION: "hive-metastore:9083"
      HADOOP_HOME: /opt/hadoop
      HIVE_HOME: /opt/hive
    ports:
      - "10000:10000"
    depends_on:
      - hive-metastore
    networks:
      - stock_network

  spark:
    image: docker.io/bitnami/spark:3.5
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - '8080:8080'
      - '7077:7077'

  spark-worker:
    image: docker.io/bitnami/spark:3.5
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    depends_on:
      - spark




networks:
  stock_network:

volumes:
  backend_data:
  kafka_data: